[{"path":[]},{"path":"/articles/Lab1.html","id":"part-1-generalization-assignment","dir":"Articles","previous_headings":"","what":"PART 1: Generalization Assignment","title":"Lab1","text":"","code":"library(readxl) library(dplyr) #  # Attaching package: 'dplyr' # The following objects are masked from 'package:stats': #  #     filter, lag # The following objects are masked from 'package:base': #  #     intersect, setdiff, setequal, union library(tidyr)  lab1data <- read_xlsx(\"C:/Users/m3gad/OneDrive/Documents/R/DBSStats2Labs/Lab1_data.xlsx\", col_names = TRUE, range = \"B3:M13\") # New names: # * A -> A...1 # * B -> B...2 # * A -> A...3 # * B -> B...4 # * A -> A...5 # * ...  #What follows here is extremely sloppy, I understand and apologize. I regretfully could not figure out how to use the (col =) and (names = \"\") using pivot_long with a dataset containing multiple layered variables as is such in this assignment. Figuring out this alternative took far too much time and stress...thankfully, I was (barely albeit successfully) able to resist tampering with the raw data in Excel, a last-resort measure that I find quite unsavory since that's a large part of what we're trying to avoid by using R in the first place. Given the time and effort spent on this disastrously sloppy code- since it yields the long-form dataset with accurate formatting that we're aiming for, and I did so without tampering with the raw data, I think I'll leave it at this and call it somewhat of a success.   c1 <- lab1data[1,] c2 <- lab1data[2,] c3 <- lab1data[3,] c4 <- lab1data[4,] c5 <- lab1data[5,] c6 <- lab1data[6,] c7 <- lab1data[7,] c8 <- lab1data[8,] c9 <- lab1data[9,] c10 <- lab1data[10,]  lab1d <- t(c(c1, c2, c3, c4, c5, c6, c7, c8, c9, c10))  lab1study <- data.frame(participant = rep(1:10, each = 12),                         context = rep(rep(c(\"Noisy\", \"Quiet\"), each = 6), 10),                         time = rep(rep(c(\"Morning\", \"Afternoon\", \"Evening\"), each = 2), 20),                         condition = rep(c(\"A\", \"B\"), 60),                         scores = t(lab1d))                          lab1study #          participant context      time condition scores # A...1              1   Noisy   Morning         A     61 # B...2              1   Noisy   Morning         B     77 # A...3              1   Noisy Afternoon         A     97 # B...4              1   Noisy Afternoon         B     97 # A...5              1   Noisy   Evening         A     89 # B...6              1   Noisy   Evening         B     94 # A...7              1   Quiet   Morning         A     87 # B...8              1   Quiet   Morning         B     87 # A...9              1   Quiet Afternoon         A     65 # B...10             1   Quiet Afternoon         B     53 # A...11             1   Quiet   Evening         A     79 # B...12             1   Quiet   Evening         B     51 # A...1.1            2   Noisy   Morning         A    100 # B...2.1            2   Noisy   Morning         B     68 # A...3.1            2   Noisy Afternoon         A     92 # B...4.1            2   Noisy Afternoon         B     57 # A...5.1            2   Noisy   Evening         A     55 # B...6.1            2   Noisy   Evening         B     85 # A...7.1            2   Quiet   Morning         A     96 # B...8.1            2   Quiet   Morning         B     88 # A...9.1            2   Quiet Afternoon         A     91 # B...10.1           2   Quiet Afternoon         B     56 # A...11.1           2   Quiet   Evening         A     90 # B...12.1           2   Quiet   Evening         B     53 # A...1.2            3   Noisy   Morning         A     70 # B...2.2            3   Noisy   Morning         B     56 # A...3.2            3   Noisy Afternoon         A     89 # B...4.2            3   Noisy Afternoon         B     78 # A...5.2            3   Noisy   Evening         A     98 # B...6.2            3   Noisy   Evening         B     73 # A...7.2            3   Quiet   Morning         A     86 # B...8.2            3   Quiet   Morning         B     86 # A...9.2            3   Quiet Afternoon         A     99 # B...10.2           3   Quiet Afternoon         B     92 # A...11.2           3   Quiet   Evening         A     67 # B...12.2           3   Quiet   Evening         B     96 # A...1.3            4   Noisy   Morning         A    100 # B...2.3            4   Noisy   Morning         B     84 # A...3.3            4   Noisy Afternoon         A     59 # B...4.3            4   Noisy Afternoon         B     85 # A...5.3            4   Noisy   Evening         A     73 # B...6.3            4   Noisy   Evening         B     74 # A...7.3            4   Quiet   Morning         A     50 # B...8.3            4   Quiet   Morning         B     62 # A...9.3            4   Quiet Afternoon         A     89 # B...10.3           4   Quiet Afternoon         B     61 # A...11.3           4   Quiet   Evening         A     85 # B...12.3           4   Quiet   Evening         B     67 # A...1.4            5   Noisy   Morning         A     93 # B...2.4            5   Noisy   Morning         B     62 # A...3.4            5   Noisy Afternoon         A     83 # B...4.4            5   Noisy Afternoon         B     53 # A...5.4            5   Noisy   Evening         A     80 # B...6.4            5   Noisy   Evening         B     78 # A...7.4            5   Quiet   Morning         A     98 # B...8.4            5   Quiet   Morning         B     69 # A...9.4            5   Quiet Afternoon         A    100 # B...10.4           5   Quiet Afternoon         B     73 # A...11.4           5   Quiet   Evening         A     69 # B...12.4           5   Quiet   Evening         B     56 # A...1.5            6   Noisy   Morning         A     70 # B...2.5            6   Noisy   Morning         B     72 # A...3.5            6   Noisy Afternoon         A     72 # B...4.5            6   Noisy Afternoon         B     99 # A...5.5            6   Noisy   Evening         A     53 # B...6.5            6   Noisy   Evening         B     70 # A...7.5            6   Quiet   Morning         A     85 # B...8.5            6   Quiet   Morning         B     60 # A...9.5            6   Quiet Afternoon         A     81 # B...10.5           6   Quiet Afternoon         B     72 # A...11.5           6   Quiet   Evening         A     65 # B...12.5           6   Quiet   Evening         B     60 # A...1.6            7   Noisy   Morning         A     85 # B...2.6            7   Noisy   Morning         B     86 # A...3.6            7   Noisy Afternoon         A     69 # B...4.6            7   Noisy Afternoon         B     67 # A...5.6            7   Noisy   Evening         A     54 # B...6.6            7   Noisy   Evening         B     90 # A...7.6            7   Quiet   Morning         A     93 # B...8.6            7   Quiet   Morning         B     53 # A...9.6            7   Quiet Afternoon         A     63 # B...10.6           7   Quiet Afternoon         B     61 # A...11.6           7   Quiet   Evening         A     74 # B...12.6           7   Quiet   Evening         B     59 # A...1.7            8   Noisy   Morning         A     51 # B...2.7            8   Noisy   Morning         B     78 # A...3.7            8   Noisy Afternoon         A     69 # B...4.7            8   Noisy Afternoon         B     59 # A...5.7            8   Noisy   Evening         A     96 # B...6.7            8   Noisy   Evening         B     90 # A...7.7            8   Quiet   Morning         A     72 # B...8.7            8   Quiet   Morning         B     61 # A...9.7            8   Quiet Afternoon         A     97 # B...10.7           8   Quiet Afternoon         B     82 # A...11.7           8   Quiet   Evening         A     57 # B...12.7           8   Quiet   Evening         B     83 # A...1.8            9   Noisy   Morning         A     70 # B...2.8            9   Noisy   Morning         B     85 # A...3.8            9   Noisy Afternoon         A     93 # B...4.8            9   Noisy Afternoon         B     87 # A...5.8            9   Noisy   Evening         A     78 # B...6.8            9   Noisy   Evening         B     64 # A...7.8            9   Quiet   Morning         A    100 # B...8.8            9   Quiet   Morning         B     98 # A...9.8            9   Quiet Afternoon         A     68 # B...10.8           9   Quiet Afternoon         B     82 # A...11.8           9   Quiet   Evening         A     55 # B...12.8           9   Quiet   Evening         B     58 # A...1.9           10   Noisy   Morning         A     50 # B...2.9           10   Noisy   Morning         B     81 # A...3.9           10   Noisy Afternoon         A     96 # B...4.9           10   Noisy Afternoon         B     90 # A...5.9           10   Noisy   Evening         A     71 # B...6.9           10   Noisy   Evening         B     83 # A...7.9           10   Quiet   Morning         A     76 # B...8.9           10   Quiet   Morning         B     64 # A...9.9           10   Quiet Afternoon         A     50 # B...10.9          10   Quiet Afternoon         B     75 # A...11.9          10   Quiet   Evening         A     56 # B...12.9          10   Quiet   Evening         B     72  #Struggle-report: Again, I was thankfully able to avoid tampering with the raw data. I ended up not looking at the solution video, so by that metric, I scored a 10/10. However, there was an enormous amount of googling involved, so if the score were really \"help needed\" then I deserve a -100/10."},{"path":"/articles/Lab1.html","id":"part-2-follow-along-with-exercise","dir":"Articles","previous_headings":"","what":"PART 2: Follow along with exercise","title":"Lab1","text":"","code":"wide_data <- data.frame(person = 1:5,                         Morning = c(1, 3, 2, 4, 3),                         Afternoon = c(3, 4, 5, 4, 7),                         Evening = c(7, 8, 7, 6, 9)) knitr::kable(wide_data) long_data <- data.frame(person = rep(1:5, each = 3),                        time_of_day = rep(c(\"Morning\", \"Afternoon\", \"Evening\"), 5),                        counts = c(1, 3, 7, 3, 4, 8, 2, 5, 7, 4, 4, 6, 3, 7, 9)) knitr::kable(long_data) person <- rep(1:5, 3) time_of_day <- rep(c(\"Morning\", \"Afternoon\", \"Evening\"), each = 5) counts <- c(1, 3, 7, 3, 4, 8, 2, 5, 7, 4, 4, 6, 3, 7, 9) test <- data.frame(person, time_of_day, counts) library(tidyr) pivot_longer(data = wide_data,              cols = !person,              names_to = \"time_of_day\",              values_to = \"counts\") # # A tibble: 15 x 3 #    person time_of_day counts #     <int> <chr>        <dbl> #  1      1 Morning          1 #  2      1 Afternoon        3 #  3      1 Evening          7 #  4      2 Morning          3 #  5      2 Afternoon        4 #  6      2 Evening          8 #  7      3 Morning          2 #  8      3 Afternoon        5 #  9      3 Evening          7 # 10      4 Morning          4 # 11      4 Afternoon        4 # 12      4 Evening          6 # 13      5 Morning          3 # 14      5 Afternoon        7 # 15      5 Evening          9 proprietary_data <- \"1, 3, 7; 3, 4, 8; 2, 5, 7; 4, 4, 6; 3, 7, 9\"  library(dplyr)  subjects <- unlist(strsplit(proprietary_data, split = \";\")) subjects # [1] \"1, 3, 7\"  \" 3, 4, 8\" \" 2, 5, 7\" \" 4, 4, 6\" \" 3, 7, 9\" subjects <- strsplit(subjects, split = \",\") subjects # [[1]] # [1] \"1\"  \" 3\" \" 7\" #  # [[2]] # [1] \" 3\" \" 4\" \" 8\" #  # [[3]] # [1] \" 2\" \" 5\" \" 7\" #  # [[4]] # [1] \" 4\" \" 4\" \" 6\" #  # [[5]] # [1] \" 3\" \" 7\" \" 9\"  subjects <- t(data.frame(subjects)) subjects #                     [,1] [,2] [,3] # c..1.....3.....7..  \"1\"  \" 3\" \" 7\" # c...3.....4.....8.. \" 3\" \" 4\" \" 8\" # c...2.....5.....7.. \" 2\" \" 5\" \" 7\" # c...4.....4.....6.. \" 4\" \" 4\" \" 6\" # c...3.....7.....9.. \" 3\" \" 7\" \" 9\"  colnames(subjects) <- c(\"Morning\", \"Afternoon\", \"Evening\") subjects #                     Morning Afternoon Evening # c..1.....3.....7..  \"1\"     \" 3\"      \" 7\"    # c...3.....4.....8.. \" 3\"    \" 4\"      \" 8\"    # c...2.....5.....7.. \" 2\"    \" 5\"      \" 7\"    # c...4.....4.....6.. \" 4\"    \" 4\"      \" 6\"    # c...3.....7.....9.. \" 3\"    \" 7\"      \" 9\"  row.names(subjects) <- 1:5  subjects <- as.data.frame(subjects) %>%    mutate(person = 1:5)  pivot_longer(data = subjects,              cols = 1:3,              names_to = \"time_of_day\",              values_to = \"counts\") # # A tibble: 15 x 3 #    person time_of_day counts #     <int> <chr>       <chr>  #  1      1 Morning     \"1\"    #  2      1 Afternoon   \" 3\"   #  3      1 Evening     \" 7\"   #  4      2 Morning     \" 3\"   #  5      2 Afternoon   \" 4\"   #  6      2 Evening     \" 8\"   #  7      3 Morning     \" 2\"   #  8      3 Afternoon   \" 5\"   #  9      3 Evening     \" 7\"   # 10      4 Morning     \" 4\"   # 11      4 Afternoon   \" 4\"   # 12      4 Evening     \" 6\"   # 13      5 Morning     \" 3\"   # 14      5 Afternoon   \" 7\"   # 15      5 Evening     \" 9\" dv <- rnorm(10,0,1) t.test(dv) #  #   One Sample t-test #  # data:  dv # t = -0.057707, df = 9, p-value = 0.9552 # alternative hypothesis: true mean is not equal to 0 # 95 percent confidence interval: #  -0.6671818  0.6339892 # sample estimates: #   mean of x  # -0.01659631 subject_data <- matrix(rbinom(50*10,1,0.5), ncol = 10, nrow = 50) subject_means <- rowMeans(subject_data) t.test(subject_means, mu=0.5) #  #   One Sample t-test #  # data:  subject_means # t = 1.5544, df = 49, p-value = 0.1265 # alternative hypothesis: true mean is not equal to 0.5 # 95 percent confidence interval: #  0.4900436 0.5779564 # sample estimates: # mean of x  #     0.534 subject_number <- rep(1:25, each=10)  day <- rep(rep(c(\"Wednesday\", \"Sunday\"), each = 5), 25)  measurement_number <- rep(1:5, 2*25)  weights <- rnorm(250, 100, 25)  weight_data <- data.frame(subject_number,                           day,                           measurement_number,                           weights) head(weight_data) #   subject_number       day measurement_number   weights # 1              1 Wednesday                  1  86.89651 # 2              1 Wednesday                  2  77.90897 # 3              1 Wednesday                  3 105.87370 # 4              1 Wednesday                  4  79.61605 # 5              1 Wednesday                  5 150.56332 # 6              1    Sunday                  1 100.14860  #Alternatively  weight_data <- data.frame(subject_number = rep(1:25, each = 10),                           day = rep(rep(c(\"Wednesday\", \"Sunday\"), each = 5), 25),                           measurement_number = rep(1:5, 2*25),                           weights = rnorm(250, 100, 25))  subject_means <- weight_data %>%   group_by(subject_number, day) %>%   summarize(mean_weight = mean(weights), .groups = \"drop\")  head(subject_means) # # A tibble: 6 x 3 #   subject_number day       mean_weight #            <int> <chr>           <dbl> # 1              1 Sunday           68.6 # 2              1 Wednesday       110.  # 3              2 Sunday           97.1 # 4              2 Wednesday       112.  # 5              3 Sunday           81.3 # 6              3 Wednesday       113.  t.test(mean_weight~day, paired=TRUE, data = subject_means) #  #   Paired t-test #  # data:  mean_weight by day # t = 0.2322, df = 24, p-value = 0.8184 # alternative hypothesis: true difference in means is not equal to 0 # 95 percent confidence interval: #  -6.834064  8.566716 # sample estimates: # mean of the differences  #               0.8663258 subjects <- rep(1:10, each = 50) room <- rep(c(\"Noisy\",\"Quiet\"), each = 50*5) words <- rep(1:50, 10) correct <- rbinom(500, 1, 0.5)  recall_data <- tibble(subjects,                       room,                       words,                       correct)  recall_data # # A tibble: 500 x 4 #    subjects room  words correct #       <int> <chr> <int>   <int> #  1        1 Noisy     1       0 #  2        1 Noisy     2       1 #  3        1 Noisy     3       1 #  4        1 Noisy     4       0 #  5        1 Noisy     5       1 #  6        1 Noisy     6       1 #  7        1 Noisy     7       1 #  8        1 Noisy     8       0 #  9        1 Noisy     9       1 # 10        1 Noisy    10       1 # # ... with 490 more rows  count_data <- recall_data %>%   group_by(subjects, room) %>%   summarize(number_correct = sum(correct), .groups = \"drop\")  count_data # # A tibble: 10 x 3 #    subjects room  number_correct #       <int> <chr>          <int> #  1        1 Noisy             26 #  2        2 Noisy             26 #  3        3 Noisy             20 #  4        4 Noisy             26 #  5        5 Noisy             23 #  6        6 Quiet             24 #  7        7 Quiet             27 #  8        8 Quiet             29 #  9        9 Quiet             26 # 10       10 Quiet             27  t.test(number_correct~room, var.equal=TRUE, data=count_data) #  #   Two Sample t-test #  # data:  number_correct by room # t = -1.6562, df = 8, p-value = 0.1363 # alternative hypothesis: true difference in means between group Noisy and group Quiet is not equal to 0 # 95 percent confidence interval: #  -5.7417175  0.9417175 # sample estimates: # mean in group Noisy mean in group Quiet  #                24.2                26.6"},{"path":"/articles/Lab1.html","id":"linear-regression","dir":"Articles","previous_headings":"","what":"Linear regression","title":"Lab1","text":"","code":"people <- tibble(height = rnorm(100, 90, 10),                  day = sample(1:31, 100, replace = TRUE)) people # # A tibble: 100 x 2 #    height   day #     <dbl> <int> #  1  106.     26 #  2   94.5    15 #  3   97.4     1 #  4   77.5    26 #  5   90.6    31 #  6   98.4     7 #  7   85.8     3 #  8   92.2    12 #  9  102.     23 # 10   84.8    29 # # ... with 90 more rows  lm.out <- lm(height~day, data = people) lm.out #  # Call: # lm(formula = height ~ day, data = people) #  # Coefficients: # (Intercept)          day   #    88.63057      0.07272  summary(lm.out) #  # Call: # lm(formula = height ~ day, data = people) #  # Residuals: #     Min      1Q  Median      3Q     Max  # -30.206  -6.302  -0.675   6.892  38.222  #  # Coefficients: #             Estimate Std. Error t value Pr(>|t|)     # (Intercept) 88.63057    2.20736  40.152   <2e-16 *** # day          0.07272    0.11632   0.625    0.533     # --- # Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #  # Residual standard error: 11.19 on 98 degrees of freedom # Multiple R-squared:  0.003972,    Adjusted R-squared:  -0.006191  # F-statistic: 0.3908 on 1 and 98 DF,  p-value: 0.5333"},{"path":[]},{"path":"/articles/Lab1.html","id":"second-level-header","dir":"Articles","previous_headings":"One-way ANOVA","what":"second level header","title":"Lab1","text":"plain text.","code":"weight_data <- tibble(subject_number = rep(1:25, each = 5*7),                       day = rep(rep(c(\"S\", \"M\", \"T\", \"W\", \"Th\", \"F\", \"Sa\"),                                     each = 5), 25),                       measurement_number = rep(1:5, 7*25),                       weights = rnorm(25*5*7, 100, 25))  subject_means <- weight_data %>%   group_by(subject_number, day) %>%   summarize(mean_weight = mean(weights), .groups = \"drop\")  subject_means # # A tibble: 175 x 3 #    subject_number day   mean_weight #             <int> <chr>       <dbl> #  1              1 F            80.4 #  2              1 M           103.  #  3              1 S            87.0 #  4              1 Sa           87.6 #  5              1 T           107.  #  6              1 Th          111.  #  7              1 W           101.  #  8              2 F           116.  #  9              2 M           119.  # 10              2 S           102.  # # ... with 165 more rows  aov.out <- aov(mean_weight ~ day, data = subject_means) summary(aov.out) #              Df Sum Sq Mean Sq F value Pr(>F) # day           6    430   71.72   0.623  0.712 # Residuals   168  19345  115.15"},{"path":"/articles/Lab1.html","id":"regression-and-anova-are-run-similarly","dir":"Articles","previous_headings":"","what":"Regression and ANOVA are run similarly","title":"Lab1","text":"","code":"subject_means <- weight_data %>%   group_by(subject_number, day) %>%   summarize(mean_weight = mean(weights), .groups=\"drop\")  subject_means # # A tibble: 175 x 3 #    subject_number day   mean_weight #             <int> <chr>       <dbl> #  1              1 F            80.4 #  2              1 M           103.  #  3              1 S            87.0 #  4              1 Sa           87.6 #  5              1 T           107.  #  6              1 Th          111.  #  7              1 W           101.  #  8              2 F           116.  #  9              2 M           119.  # 10              2 S           102.  # # ... with 165 more rows  lm.out <- lm(mean_weight ~ day, data = subject_means) summary(lm.out) #  # Call: # lm(formula = mean_weight ~ day, data = subject_means) #  # Residuals: #     Min      1Q  Median      3Q     Max  # -25.853  -6.216   0.570   6.813  27.695  #  # Coefficients: #              Estimate Std. Error t value Pr(>|t|)     # (Intercept) 101.46617    2.14612  47.279   <2e-16 *** # dayM         -1.38963    3.03507  -0.458    0.648     # dayS         -0.07779    3.03507  -0.026    0.980     # daySa         1.55380    3.03507   0.512    0.609     # dayT          1.05025    3.03507   0.346    0.730     # dayTh         2.14367    3.03507   0.706    0.481     # dayW         -2.68566    3.03507  -0.885    0.377     # --- # Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #  # Residual standard error: 10.73 on 168 degrees of freedom # Multiple R-squared:  0.02176, Adjusted R-squared:  -0.01318  # F-statistic: 0.6228 on 6 and 168 DF,  p-value: 0.7118"},{"path":"/articles/Lab1.html","id":"factorial-anova","dir":"Articles","previous_headings":"","what":"Factorial ANOVA","title":"Lab1","text":"","code":"weight_data <- tibble(subject_number = rep(1:25, each = 4*7),                       day = rep(rep(c(\"S\",\"M\",\"T\",\"W\",\"Th\",\"F\",\"Sa\"),                                     each = 4), 25),                       time_of_day = rep(c(\"Morning\", \"Morning\", \"Evening\", \"Evening\"), 7*25),                       measurement_number = rep(rep(1:2, 2), 7*25),                       weights = rnorm(25*4*7, 100, 25))  subject_means <- weight_data %>%   group_by(subject_number, day, time_of_day) %>%   summarize(mean_weight = mean(weights), .groups = \"drop\")  subject_means # # A tibble: 350 x 4 #    subject_number day   time_of_day mean_weight #             <int> <chr> <chr>             <dbl> #  1              1 F     Evening           123.  #  2              1 F     Morning            84.1 #  3              1 M     Evening            78.6 #  4              1 M     Morning           112.  #  5              1 S     Evening           119.  #  6              1 S     Morning            77.9 #  7              1 Sa    Evening            89.1 #  8              1 Sa    Morning           106.  #  9              1 T     Evening            92.0 # 10              1 T     Morning           108.  # # ... with 340 more rows  aov.out <- aov(mean_weight ~ day*time_of_day, data = subject_means) summary(aov.out) #                  Df Sum Sq Mean Sq F value Pr(>F) # day               6   2513   418.8   1.358  0.231 # time_of_day       1    121   121.0   0.392  0.531 # day:time_of_day   6   1463   243.8   0.791  0.577 # Residuals       336 103585   308.3"},{"path":"/articles/Lab1.html","id":"switching-from-factorial-anova-to-multiple-regression","dir":"Articles","previous_headings":"","what":"Switching from Factorial ANOVA to multiple regression","title":"Lab1","text":"","code":"subject_means <- weight_data %>%    group_by(subject_number, day, time_of_day) %>%   summarize(mean_weight = mean(weights), .groups = \"drop\")  subject_means$day <- as.factor(subject_means$day) subject_means$time_of_day <- as.factor(subject_means$time_of_day)  subject_means # # A tibble: 350 x 4 #    subject_number day   time_of_day mean_weight #             <int> <fct> <fct>             <dbl> #  1              1 F     Evening           123.  #  2              1 F     Morning            84.1 #  3              1 M     Evening            78.6 #  4              1 M     Morning           112.  #  5              1 S     Evening           119.  #  6              1 S     Morning            77.9 #  7              1 Sa    Evening            89.1 #  8              1 Sa    Morning           106.  #  9              1 T     Evening            92.0 # 10              1 T     Morning           108.  # # ... with 340 more rows  lm.out <- lm(mean_weight ~ day*time_of_day, data = subject_means) summary(lm.out) #  # Call: # lm(formula = mean_weight ~ day * time_of_day, data = subject_means) #  # Residuals: #     Min      1Q  Median      3Q     Max  # -46.506 -12.563  -0.457  12.385  43.232  #  # Coefficients: #                          Estimate Std. Error t value Pr(>|t|)     # (Intercept)              108.4934     3.5116  30.895  < 2e-16 *** # dayM                      -7.2818     4.9662  -1.466  0.14351     # dayS                     -14.1963     4.9662  -2.859  0.00452 **  # daySa                     -8.8049     4.9662  -1.773  0.07714 .   # dayT                     -11.7871     4.9662  -2.373  0.01818 *   # dayTh                     -8.8137     4.9662  -1.775  0.07685 .   # dayW                      -8.9177     4.9662  -1.796  0.07345 .   # time_of_dayMorning        -4.9668     4.9662  -1.000  0.31797     # dayM:time_of_dayMorning    6.5030     7.0233   0.926  0.35515     # dayS:time_of_dayMorning   11.5640     7.0233   1.647  0.10059     # daySa:time_of_dayMorning   7.4240     7.0233   1.057  0.29125     # dayT:time_of_dayMorning   10.7224     7.0233   1.527  0.12778     # dayTh:time_of_dayMorning   0.9378     7.0233   0.134  0.89386     # dayW:time_of_dayMorning    5.8478     7.0233   0.833  0.40564     # --- # Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #  # Residual standard error: 17.56 on 336 degrees of freedom # Multiple R-squared:  0.03805, Adjusted R-squared:  0.0008279  # F-statistic: 1.022 on 13 and 336 DF,  p-value: 0.4291  anova(lm.out) # Analysis of Variance Table #  # Response: mean_weight #                  Df Sum Sq Mean Sq F value Pr(>F) # day               6   2513  418.81  1.3585 0.2308 # time_of_day       1    121  121.00  0.3925 0.5314 # day:time_of_day   6   1463  243.85  0.7910 0.5775 # Residuals       336 103585  308.29"},{"path":[]},{"path":"/articles/Lab2.html","id":"part-1-generalization-exercise","dir":"Articles","previous_headings":"","what":"Part 1: Generalization Exercise","title":"Lab2","text":"","code":"# Sample Code: library(tibble) library(ggplot2) slamecka_design <- tibble(number_of_learning_trials = rep(c(2,4,8), each = 6),                           number_of_IL = rep(rep(c(2,4,8), 2), 3),                           subjects = 1:18,                           recall = c(35, 21, 6,                                      39, 31, 8,                                      40, 34, 18,                                      52, 42, 26,                                      61, 58, 46,                                      73, 66, 52                                      )                           ) ggplot(slamecka_design, aes(x = number_of_IL,                             group = number_of_learning_trials,                             y=recall))+   geom_line(stat = \"summary\", fun = \"mean\")+   geom_point(stat = \"summary\", fun = \"mean\")+   theme_classic() # original version:"},{"path":"/articles/Lab2.html","id":"question-1-make-this-graph-resemble-that-from-textbook-fig--5-5-as-closely-as-possible","dir":"Articles","previous_headings":"","what":"Question 1) Make this graph resemble that from textbook fig.Â 5.5 as closely as possible:","title":"Lab2","text":"","code":"slamecka_design$number_of_learning_trials <- as.factor(slamecka_design$number_of_learning_trials)  ggplot(slamecka_design, aes(x = number_of_IL,                             group = number_of_learning_trials,                             y=recall))+   geom_line(stat = \"summary\", fun = \"mean\")+   geom_point(stat = \"summary\", fun = \"mean\", aes(shape = number_of_learning_trials))+   theme_classic()+   labs(y=\"Number of words correct\", x = \"Number of interpolated lists\")+   scale_y_continuous(breaks = c(0, 20, 40, 60, 80), limits = c(0,80))+   scale_x_continuous(breaks = c(2, 4, 8), limits = c(2, 8)) # Struggle-report: I was able to assign the labels and read just which variables were on which axes correctly with minimal help (only slight googling), and figured out the shape differentiation with aes, but once it came to skipping the tick-mark for \"6\" on the x-axis and splitting the legend to address each line individually (*\"splitting the legend\" turned out to be something you don't care about, so never-mind on that), I needed to check the solutions video. Let's call this a 6/10."},{"path":"/articles/Lab2.html","id":"problem-2-add-a-third-variable-amount-of-reward-given","dir":"Articles","previous_headings":"","what":"Problem 2: Add a third variable (amount of reward given)","title":"Lab2","text":"","code":"library(ggplot2) library(dplyr) #   #  Attaching package: 'dplyr' #  The following objects are masked from 'package:stats': #   #      filter, lag #  The following objects are masked from 'package:base': #   #      intersect, setdiff, setequal, union library(tibble) slamecka_design_modded <- tibble(reward = rep(c(\"A:$0\", \"B:$50\", \"c:$1,000,000\"), each = 9),                           practice = rep(rep(c(2,4,8), each = 3), 3),                           distraction = as.factor(rep(c(0,4,8), 9)),                           recall = c(5, 3, 1,                                      6, 4, 2,                                      7, 5, 3,                                      10, 8, 6,                                      11, 9, 7,                                      12, 10, 8,                                      15, 13, 11,                                      16, 14, 12,                                      17, 15, 13                                      )                           )  ggplot(slamecka_design_modded, aes(x = practice,                                    group = distraction,                                    y = recall,                                    shape = distraction))+   geom_line()+   geom_point()+   theme_classic()+   scale_y_continuous(breaks = c(0, 5, 10, 15, 20), limits = c(0,20))+   scale_x_continuous(breaks = c(2, 4, 8))+   labs(y = \"Recall\", x = \"Amount of Practice\")+                      facet_wrap(~reward) # Struggle-report: While I think I know in theory how to add the \"reward\" variable, I don't think it's clear enough from the question exactly how I should be distributing/arranging/counterbalancing it (with \"rep\") to line up with the data. Trying not to spoil anything else while checking this in the video...Ok, I thought I'd be able to do it but needed to consult the video again. I know it must have something to do with facet-wrapping. 3/10, maaaybe 4/10 on help required here."},{"path":"/articles/Lab2.html","id":"part-2-follow-along-with-lab-demonstration","dir":"Articles","previous_headings":"","what":"Part 2: Follow-along with lab demonstration","title":"Lab2","text":"","code":"# Unpaired-samples t-test library(tibble) simple_design <- tibble(group = rep(c(0,1), each = 10),                        DV = c(1, 3, 2, 4, 3, 4, 5, 6, 5, 4, 5, 4, 3, 2, 3, 4, 5, 6, 8, 9)) knitr::kable(simple_design) library(ggplot2)  ggplot(simple_design, aes(x=group, y=DV))+   geom_bar(stat = \"summary\", fun = \"mean\", position = \"dodge\")+   geom_point()+   geom_smooth(method = \"lm\", se = FALSE) #  `geom_smooth()` using formula 'y ~ x' t.test(DV~group, var.equal=TRUE, data = simple_design) #   #   Two Sample t-test #   #  data:  DV by group #  t = -1.412, df = 18, p-value = 0.175 #  alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0 #  95 percent confidence interval: #   -2.9854404  0.5854404 #  sample estimates: #  mean in group 0 mean in group 1  #              3.7             4.9"},{"path":"/articles/Lab2.html","id":"regression-analysis-on-the-same-data-and-contextualized-version","dir":"Articles","previous_headings":"","what":"Regression analysis on the same data, and contextualized version","title":"Lab2","text":"","code":"lm(DV~group, data=simple_design) #   #  Call: #  lm(formula = DV ~ group, data = simple_design) #   #  Coefficients: #  (Intercept)        group   #          3.7          1.2 summary(lm(DV~group, data=simple_design)) #   #  Call: #  lm(formula = DV ~ group, data = simple_design) #   #  Residuals: #     Min     1Q Median     3Q    Max  #   -2.90  -1.10   0.10   1.15   4.10  #   #  Coefficients: #              Estimate Std. Error t value Pr(>|t|)     #  (Intercept)   3.7000     0.6009   6.157 8.18e-06 *** #  group         1.2000     0.8498   1.412    0.175     #  --- #  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #   #  Residual standard error: 1.9 on 18 degrees of freedom #  Multiple R-squared:  0.09972,    Adjusted R-squared:  0.04971  #  F-statistic: 1.994 on 1 and 18 DF,  p-value: 0.175  recall_design <- tibble(practice = rep(c(2, 4, 8), each = 3),                         subjects = 1:9,                         recall = c(5, 7, 8,                                     8, 10, 12,                                    12, 15, 17)) knitr::kable(simple_design) ggplot(recall_design, aes(x=practice, y=recall))+   geom_bar(stat = \"summary\", fun = \"mean\", position = \"dodge\")+   geom_point()+   geom_smooth(method = \"lm\", formula = y~x, se = FALSE) summary(lm(recall~practice, data = recall_design)) #   #  Call: #  lm(formula = recall ~ practice, data = recall_design) #   #  Residuals: #      Min      1Q  Median      3Q     Max  #  -2.8095 -1.5714  0.1905  1.0476  2.4286  #   #  Coefficients: #              Estimate Std. Error t value Pr(>|t|)    #  (Intercept)   4.3333     1.3678   3.168  0.01575 *  #  practice      1.3095     0.2585   5.066  0.00145 ** #  --- #  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #   #  Residual standard error: 1.934 on 7 degrees of freedom #  Multiple R-squared:  0.7857, Adjusted R-squared:  0.7551  #  F-statistic: 25.67 on 1 and 7 DF,  p-value: 0.001453  # Making it categorical rather than continuous recall_design$practice <- as.factor(recall_design$practice)  ggplot(recall_design, aes(x=practice, y=recall))+   geom_bar(stat = \"summary\", fun = \"mean\", position = \"dodge\")+   geom_point()+   geom_smooth(method = \"lm\", formula = y~x, se = FALSE) summary(lm(recall~practice, data = recall_design)) #   #  Call: #  lm(formula = recall ~ practice, data = recall_design) #   #  Residuals: #      Min      1Q  Median      3Q     Max  #  -2.6667 -1.6667  0.3333  1.3333  2.3333  #   #  Coefficients: #              Estimate Std. Error t value Pr(>|t|)    #  (Intercept)    6.667      1.186   5.620  0.00136 ** #  practice4      3.333      1.678   1.987  0.09413 .  #  practice8      8.000      1.678   4.768  0.00310 ** #  --- #  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #   #  Residual standard error: 2.055 on 6 degrees of freedom #  Multiple R-squared:  0.7927, Adjusted R-squared:  0.7236  #  F-statistic: 11.47 on 2 and 6 DF,  p-value: 0.008905 # The analysis as an ANOVA? summary(aov(recall~practice, data = recall_design)) #              Df Sum Sq Mean Sq F value Pr(>F)    #  practice     2  96.89   48.44   11.47 0.0089 ** #  Residuals    6  25.33    4.22                   #  --- #  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"/articles/Lab2.html","id":"copying-the-slamecka-1960-design","dir":"Articles","previous_headings":"","what":"Copying the Slamecka (1960) Design","title":"Lab2","text":"","code":"slamecka_design <- tibble(number_of_learning_trials = rep(c(2, 4, 8), each = 6),                           number_of_IL = rep(rep(c(2, 4, 8), 2), 3),                           subjects = 1:18,                           recall = c(35, 21, 6,                                      39, 51, 8,                                      40, 34, 18,                                      52, 42, 26,                                      61, 58, 46,                                      73, 66, 52                                      )                           ) knitr::kable(slamecka_design) ggplot(slamecka_design, aes(x=number_of_IL,                             group = number_of_learning_trials,                             y = recall))+   geom_line(stat = \"summary\", fun = \"mean\")+   geom_point(stat = \"summary\", fun = \"mean\")+   theme_classic() # Running the Analysis lm(recall~number_of_learning_trials + number_of_IL, data = slamecka_design) #   #  Call: #  lm(formula = recall ~ number_of_learning_trials + number_of_IL,  #      data = slamecka_design) #   #  Coefficients: #                (Intercept)  number_of_learning_trials   #                     33.889                      5.524   #               number_of_IL   #                     -4.119 summary(lm(recall~number_of_learning_trials + number_of_IL, data = slamecka_design)) #   #  Call: #  lm(formula = recall ~ number_of_learning_trials + number_of_IL,  #      data = slamecka_design) #   #  Residuals: #      Min      1Q  Median      3Q     Max  #  -8.8413 -5.3889 -0.4127  3.1111 22.5397  #   #  Coefficients: #                            Estimate Std. Error t value Pr(>|t|)     #  (Intercept)                33.8889     5.2797   6.419 1.16e-05 *** #  number_of_learning_trials   5.5238     0.7483   7.382 2.29e-06 *** #  number_of_IL               -4.1190     0.7483  -5.504 6.06e-05 *** #  --- #  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #   #  Residual standard error: 7.92 on 15 degrees of freedom #  Multiple R-squared:  0.8497, Adjusted R-squared:  0.8296  #  F-statistic: 42.39 on 2 and 15 DF,  p-value: 6.725e-07  ggplot(slamecka_design, aes(x=number_of_IL,                             group = number_of_learning_trials,                             y=recall))+   geom_line(stat = \"summary\", fun = \"mean\")+   geom_point(stat = \"summary\", fun = \"mean\")+   theme_classic() slamecka_design$number_of_IL <- as.factor(slamecka_design$number_of_IL)  ggplot(slamecka_design, aes(x = number_of_learning_trials,                             group = number_of_IL,                             y = recall))+   geom_line(stat = \"summary\", fun = \"mean\")+   geom_point(stat = \"summary\", fun = \"mean\", aes(shape = number_of_IL))+   theme_classic() # Facet-wrapping ggplot(slamecka_design, aes(x = number_of_learning_trials,                             y = recall))+   geom_line(stat = \"summary\", fun = \"mean\")+   geom_point(stat = \"summary\", fun = \"mean\")+   theme_classic()+   facet_wrap(~number_of_IL) slamecka_design <- tibble(number_of_learning_trials = rep(c(2, 4, 8), each = 6),                           number_of_IL = rep(rep(c(2, 4, 8), 2), 3),                           subjects = rep(1:6, each = 3),                           recall = c(35, 21, 6,                                      39, 51, 8,                                      40, 34, 18,                                      52, 42, 26,                                      61, 58, 46,                                      73, 66, 52                                      )                           ) knitr::kable(slamecka_design) ggplot(slamecka_design, aes(x = number_of_IL,                              y = recall))+   geom_line()+   geom_point()+   theme_classic()+   facet_wrap(~number_of_learning_trials*subjects, ncol=2)"},{"path":[]},{"path":"/articles/Lab3.html","id":"part-1-generalization-assignment","dir":"Articles","previous_headings":"","what":"PART 1: Generalization Assignment","title":"Lab3","text":"","code":"#NOTE: Because I'm not quite brilliant enough to creatively think of tutorial tips 100% on my own, I had to consult some online discourse. The links to these web-pages are provided below. Matt- please let me know if this was an acceptable choice. #When conducting multiple regression analyses, it is crucial that all predictor variables are #orthogonal. The \"geometric\" basis for this concept may obfuscate the straightforwardness of #this concept: each predictor should be independent from one another, avoid predicting changes  #in other predictor variables, and should hold an independent predictive \"influence\" over the  #dependent variables. In other words- if any predictor varies alongside another, their shared #influence over the DV would confound one another. For example, if \"height\" and \"age\" were both #assessed as predictive variables for the DV \"joint health,\" the principle of orthogonality would #be inherently violated since height and age could both be reasonably hypothesized as holding a  #relationship with joint health- and because height would also be expected to covary with age. #Any unique influence of height on joint health, for example, might accidentally be confounded  #by the non-orthogonal influence of age on height and consequently joint health as well.  #Taken at face value, checking for orthogonality between two variables is extremely simple. #Because non-orthogonal predictor variables would co-vary with one another, it is reasonable #to deduce that they should correlate with one another as well. Let's flesh out this age/height/ #joint-health example with some fake data assuming 18 subjects (and some arbitrary \"joint health\" metric):  library(tibble) age <- c(8, 10, 11, 13, 14, 16, 19, 21, 23, 26, 29, 32, 41, 48, 52, 59, 66, 74) height <- c(4.5, 4.8, 4.7, 5.3, 5.5, 6, 5.9, 6.2, 5.5, 6.3, 5.6, 5.8, 5.6, 6.2, 4.9, 5.7, 6.1, 5.9) joint_health_index <- c(7, 8, 9, 8, 6, 5, 7, 6, 8, 3, 8, 6, 8, 5, 2, 2, 3, 2)  joint_health_data <- tibble(Age = age,                             Height = height,                             JHI = joint_health_index)  #We can effectively check for orthogonality by assessing whether age and height are correlated with one another, using a simple cor expression.  cor(joint_health_data) #                Age     Height        JHI #  Age     1.0000000  0.4052203 -0.7520185 #  Height  0.4052203  1.0000000 -0.4203486 #  JHI    -0.7520185 -0.4203486  1.0000000  #It is apparent from this correlation matrix that age and height have a correlation of 0.40. Now what about the R^2 values?  cor(joint_health_data)^2 #               Age    Height       JHI #  Age    1.0000000 0.1642035 0.5655318 #  Height 0.1642035 1.0000000 0.1766930 #  JHI    0.5655318 0.1766930 1.0000000  #Thus, 16.4% of variation in either the age or height variables can explain variation in height or age, respectively. They are therefore liable to confound one another in regression analyses for their ability to predict variance in joint health.   #There are methods to correct for non-orthogonality such as this, however that is not the focus of the present tutorial. Instead, it is my desire to point out that manually checking correlation matrices is a relatively indirect and (debatably) inefficient way to probe for orthogonality in one's data-set. Let's talk about one other method to check orthogonality. We must install and load the \"ibd\" package, and our data must be in matrix format if it is not already. We can then use the \"check.orthogonality\" function.  data <- as.matrix(joint_health_data)  install.packages(\"ibd\", repos = \"http://cran.us.r-project.org\") #  Installing package into 'C:/Users/m3gad/AppData/Local/Temp/Rtmp6TYvlZ/temp_libpath48906437a38' #  (as 'lib' is unspecified) #  Warning: unable to access index for repository http://cran.us.r-project.org/src/contrib: #    cannot open URL 'http://cran.us.r-project.org/src/contrib/PACKAGES' #  Warning: package 'ibd' is not available for this version of R #   #  A version of this package for your version of R might be available elsewhere, #  see the ideas at #  https://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Installing-packages library(ibd) check.orthogonality(data) #  [1] 0 #This function is very simple. If the function returns a \"1,\" the rows are orthogonal. If it returns \"0,\" they are not. The limitation to this function is that it only checks whether ROWS are PAIRWISE orthogonal or not. The result returned of \"0\" reaffirms our earlier checking of correlation matrices, which also indicated that the variables are not orthogonal.   #Matt: I honestly worry that I either misunderstood this function, or that my data is not in the correct format for row-based pairwise orthogonality testing. I admit that I'm only just scratching the surface with this, but I hope this \"tutorial\" sets out what it was meant to do. #WEBPAGES REFERENCED FOR THIS DOCUMENTATION: # https://www.rdocumentation.org/packages/ibd/versions/1.5/topics/check.orthogonality # https://rdrr.io/cran/ibd/man/check.orthogonality.html"},{"path":"/articles/Lab3.html","id":"part-2-follow-along-with-lab-demonstration","dir":"Articles","previous_headings":"","what":"PART 2: Follow-along with lab demonstration","title":"Lab3","text":"#Hulme et al (1984) example","code":"#Explaining variance using multiple variables random_vectors <- matrix(rnorm(20*26, 0, 1), nrow = 20, ncol = 26) colnames(random_vectors) <- letters random_vectors <- as.data.frame(random_vectors)  hist(cor(random_vectors)) summary(lm(a~b, data = random_vectors)) #   #  Call: #  lm(formula = a ~ b, data = random_vectors) #   #  Residuals: #      Min      1Q  Median      3Q     Max  #  -1.5146 -0.6226  0.0319  0.6225  1.5195  #   #  Coefficients: #              Estimate Std. Error t value Pr(>|t|) #  (Intercept)   0.1240     0.1881   0.659    0.518 #  b            -0.1958     0.1581  -1.238    0.232 #   #  Residual standard error: 0.8272 on 18 degrees of freedom #  Multiple R-squared:  0.0785, Adjusted R-squared:  0.02731  #  F-statistic: 1.533 on 1 and 18 DF,  p-value: 0.2315  summary(lm(a~b,data=random_vectors))$r.squared #  [1] 0.07850218  summary(lm(a~b,data=random_vectors))$r.squared #  [1] 0.07850218  summary(lm(a~b+c,data=random_vectors))$r.squared #  [1] 0.1218066  summary(lm(a~b+c+d,data=random_vectors))$r.squared #  [1] 0.1652491  summary(lm(a~b+c+d+e,data=random_vectors))$r.squared #  [1] 0.2270401  summary(lm(a~b+c+d+e+f,data=random_vectors))$r.squared #  [1] 0.2307958  summary(lm(a~b+c+d+e+f+g,data=random_vectors))$r.squared #  [1] 0.2374345  summary(lm(a~b+c+d+e+f+g+h,data=random_vectors))$r.squared #  [1] 0.2407677  summary(lm(a~b*c,data=random_vectors))$r.squared #  [1] 0.2498835  summary(lm(a~b*c*d,data=random_vectors))$r.squared #  [1] 0.5066712  summary(lm(a~b*c*d*e,data=random_vectors))$r.squared #  [1] 0.8517942  summary(lm(a~b*c*d*e*f,data=random_vectors))$r.squared #  [1] 1  summary(lm(a~b*c*d*e, data = random_vectors)) #   #  Call: #  lm(formula = a ~ b * c * d * e, data = random_vectors) #   #  Residuals: #          1         2         3         4         5         6         7         8  #  -0.175119 -0.049350 -0.334371  0.586639 -0.035549  0.207243  0.793439 -0.714835  #          9        10        11        12        13        14        15        16  #  -0.104709 -0.101903 -0.173901 -0.059888  0.197234  0.110696 -0.309016 -0.009687  #         17        18        19        20  #   0.177185  0.002696  0.188888 -0.195691  #   #  Coefficients: #              Estimate Std. Error t value Pr(>|t|) #  (Intercept)   0.6216     0.3186   1.951    0.123 #  b            -0.6297     0.4423  -1.424    0.228 #  c             0.3327     0.4003   0.831    0.453 #  d            -0.7238     0.6318  -1.146    0.316 #  e             1.3574     0.9807   1.384    0.239 #  b:c          -1.5826     0.8489  -1.864    0.136 #  b:d           0.7912     0.5068   1.561    0.194 #  c:d          -0.2877     0.7201  -0.400    0.710 #  b:e           0.8415     1.0176   0.827    0.455 #  c:e           0.7395     0.7158   1.033    0.360 #  d:e          -2.2548     1.4907  -1.513    0.205 #  b:c:d         0.5083     1.1514   0.441    0.682 #  b:c:e        -0.4402     1.2072  -0.365    0.734 #  b:d:e        -0.2829     1.1121  -0.254    0.812 #  c:d:e         2.1232     1.9312   1.099    0.333 #  b:c:d:e       3.9991     2.6004   1.538    0.199 #   #  Residual standard error: 0.7038 on 4 degrees of freedom #  Multiple R-squared:  0.8518, Adjusted R-squared:  0.296  #  F-statistic: 1.533 on 15 and 4 DF,  p-value: 0.366  summary(lm(a~b*c*d*e*f, data = random_vectors)) #   #  Call: #  lm(formula = a ~ b * c * d * e * f, data = random_vectors) #   #  Residuals: #  ALL 20 residuals are 0: no residual degrees of freedom! #   #  Coefficients: (12 not defined because of singularities) #              Estimate Std. Error t value Pr(>|t|) #  (Intercept)    4.642        NaN     NaN      NaN #  b             -3.649        NaN     NaN      NaN #  c              0.287        NaN     NaN      NaN #  d             -9.315        NaN     NaN      NaN #  e             31.249        NaN     NaN      NaN #  f              5.813        NaN     NaN      NaN #  b:c          -19.267        NaN     NaN      NaN #  b:d            9.294        NaN     NaN      NaN #  c:d          -11.932        NaN     NaN      NaN #  b:e           45.111        NaN     NaN      NaN #  c:e           -8.315        NaN     NaN      NaN #  d:e           40.043        NaN     NaN      NaN #  b:f           17.218        NaN     NaN      NaN #  c:f           -2.458        NaN     NaN      NaN #  d:f           18.529        NaN     NaN      NaN #  e:f            1.941        NaN     NaN      NaN #  b:c:d          6.263        NaN     NaN      NaN #  b:c:e        -61.266        NaN     NaN      NaN #  b:d:e         24.496        NaN     NaN      NaN #  c:d:e        -43.594        NaN     NaN      NaN #  b:c:f             NA         NA      NA       NA #  b:d:f             NA         NA      NA       NA #  c:d:f             NA         NA      NA       NA #  b:e:f             NA         NA      NA       NA #  c:e:f             NA         NA      NA       NA #  d:e:f             NA         NA      NA       NA #  b:c:d:e           NA         NA      NA       NA #  b:c:d:f           NA         NA      NA       NA #  b:c:e:f           NA         NA      NA       NA #  b:d:e:f           NA         NA      NA       NA #  c:d:e:f           NA         NA      NA       NA #  b:c:d:e:f         NA         NA      NA       NA #   #  Residual standard error: NaN on 0 degrees of freedom #  Multiple R-squared:      1,  Adjusted R-squared:    NaN  #  F-statistic:   NaN on 19 and 0 DF,  p-value: NA  library(tibble)  slamecka_design <- tribble(   ~Subjects, ~OL, ~IL,   1, 2, 0,   1, 4, 4,   1, 8, 8,   2, 4, 0,   2, 8, 4,   2, 2, 8,   3, 8, 0,   3, 2, 4,   3, 4, 8,   4, 2, 4,   4, 4, 0,   4, 8, 8,   5, 4, 4,   5, 2, 8,   5, 8, 0,   6, 8, 4,   6, 4, 8,   6, 2, 0,   7, 2, 8,   7, 4, 0,   7, 8, 4,   8, 4, 8,   8, 2, 4,   8, 8, 0,   9, 8, 8,   9, 4, 4,   9, 2, 0 )  cor(slamecka_design) #           Subjects OL IL #  Subjects        1  0  0 #  OL              0  1  0 #  IL              0  0  1  slamecka_confounded <- tribble(   ~Subjects, ~OL, ~IL,   1, 2, 0,   1, 4, 4,   1, 8, 8,   2, 4, 4,   2, 8, 8,   2, 2, 0,   3, 8, 8,   3, 2, 0,   3, 4, 4,   4, 2, 0,   4, 4, 4,   4, 8, 8,   5, 4, 4,   5, 2, 0,   5, 8, 8,   6, 8, 8,   6, 4, 4,   6, 2, 0,   7, 2, 0,   7, 4, 4,   7, 8, 8,   8, 4, 4,   8, 2, 0,   8, 8, 8,   9, 8, 8,   9, 4, 4,   9, 2, 0 ) cor(slamecka_confounded) #           Subjects        OL        IL #  Subjects        1 0.0000000 0.0000000 #  OL              0 1.0000000 0.9819805 #  IL              0 0.9819805 1.0000000  library(dplyr) #   #  Attaching package: 'dplyr' #  The following objects are masked from 'package:stats': #   #      filter, lag #  The following objects are masked from 'package:base': #   #      intersect, setdiff, setequal, union library(ggplot2) data <- tibble(X = c(4, 4, 7, 7, 10, 10),                T = c(1, 2, 2, 4, 3, 6),                Y = c(14, 23, 30, 50, 39, 67))  (overall_model <- summary(lm(Y~X+T, data = data))) #   #  Call: #  lm(formula = Y ~ X + T, data = data) #   #  Residuals: #       1      2      3      4      5      6  #  -1.167 -1.667  2.333  3.333 -1.167 -1.667  #   #  Coefficients: #              Estimate Std. Error t value Pr(>|t|)    #  (Intercept)    1.667      3.598   0.463  0.67470    #  X              1.000      0.725   1.379  0.26162    #  T              9.500      1.087   8.736  0.00316 ** #  --- #  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #   #  Residual standard error: 2.877 on 3 degrees of freedom #  Multiple R-squared:  0.9866, Adjusted R-squared:  0.9776  #  F-statistic: 110.1 on 2 and 3 DF,  p-value: 0.001559  cor(data) #            X         T         Y #  X 1.0000000 0.7500000 0.8027961 #  T 0.7500000 1.0000000 0.9889517 #  Y 0.8027961 0.9889517 1.0000000 cor(data)^2 #            X         T         Y #  X 1.0000000 0.5625000 0.6444815 #  T 0.5625000 1.0000000 0.9780254 #  Y 0.6444815 0.9780254 1.0000000  lm.x <- lm(Y~X, data = data) data <- data %>%   mutate(X_residuals = residuals(lm.x),          X_predicted_Y = predict(lm.x))  knitr::kable(data) A <- ggplot(data, aes(y=Y, x=X))+   geom_point()+   geom_smooth(method=\"lm\", se=FALSE)  B <- ggplot(data, aes(y=X_predicted_Y, x=X))+   geom_point()+   geom_smooth(method=\"lm\",se=FALSE)  C <- ggplot(data, aes(y=X_residuals, x=X))+   geom_point()+   geom_smooth(method=\"lm\",se=FALSE)  install.packages(\"patchwork\", repos = \"http://cran.us.r-project.org\") #  Installing package into 'C:/Users/m3gad/AppData/Local/Temp/Rtmp6TYvlZ/temp_libpath48906437a38' #  (as 'lib' is unspecified) #  package 'patchwork' successfully unpacked and MD5 sums checked #   #  The downloaded binary packages are in #   C:\\Users\\m3gad\\AppData\\Local\\Temp\\RtmpWeF8OU\\downloaded_packages library(patchwork)  A+B+C #  `geom_smooth()` using formula 'y ~ x' #  `geom_smooth()` using formula 'y ~ x' #  `geom_smooth()` using formula 'y ~ x' lm.t <- lm(Y~T, data=data)  data <- data %>%   mutate(T_residuals = residuals(lm.t),          T_predicted_Y = predict(lm.t)) D <- ggplot(data, aes(y=Y, x=T))+   geom_point()+   geom_smooth(method=\"lm\", se=FALSE)  E <- ggplot(data, aes(y=T_predicted_Y, x=T))+   geom_point()+   geom_smooth(method=\"lm\",se=FALSE)  F <- ggplot(data, aes(y=T_residuals, x=T))+   geom_point()+   geom_smooth(method=\"lm\",se=FALSE)  D+E+F #  `geom_smooth()` using formula 'y ~ x' #  `geom_smooth()` using formula 'y ~ x' #  `geom_smooth()` using formula 'y ~ x' lm.xt <- lm(X~T, data = data) residuals(lm.xt) #       1      2      3      4      5      6  #  -0.750 -1.875  1.125 -1.125  3.000 -0.375  cor(residuals(lm.xt), data$Y)^2 #  [1] 0.008528111  lm.tx <- lm(T~X, data = data) residuals(lm.tx) #     1    2    3    4    5    6  #  -0.5  0.5 -1.0  1.0 -1.5  1.5 cor(residuals(lm.tx), data$Y)^2 #  [1] 0.342072  overall_model$r.squared - cor(residuals(lm.xt), data$Y)^2 - cor(residuals(lm.tx), data$Y)^2 #  [1] 0.6359534  library(ppcor) #  Loading required package: MASS #   #  Attaching package: 'MASS' #  The following object is masked from 'package:patchwork': #   #      area #  The following object is masked from 'package:dplyr': #   #      select data <- tibble(X = c(4, 4, 7, 7, 10, 10),                T = c(1, 2, 2, 4, 3, 6),                Y = c(14, 23, 30, 50, 39, 67))  spcor(data, method = \"pearson\") #  $estimate #              X          T         Y #  X  1.00000000 -0.2963241 0.4120552 #  T -0.07367089  1.0000000 0.6488088 #  Y  0.09234777  0.5848692 1.0000000 #   #  $p.value #            X         T         Y #  X 0.0000000 0.6283051 0.4906046 #  T 0.9062842 0.0000000 0.2362282 #  Y 0.8825865 0.3002769 0.0000000 #   #  $statistic #             X          T         Y #  X  0.0000000 -0.5373837 0.7832889 #  T -0.1279494  0.0000000 1.4767956 #  Y  0.1606375  1.2489073 0.0000000 #   #  $n #  [1] 6 #   #  $gp #  [1] 1 #   #  $method #  [1] \"pearson\"  spcor(data, method = \"pearson\")$estimate^2 #              X          T         Y #  X 1.000000000 0.08780798 0.1697895 #  T 0.005427400 1.00000000 0.4209528 #  Y 0.008528111 0.34207202 1.0000000"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"package maintainer. Maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Ww (2022). DBSStats2Labs: Package (Title Case). R package version 0.1.0.","code":"@Manual{,   title = {DBSStats2Labs: What the Package Does (Title Case)},   author = {Who wrote it},   year = {2022},   note = {R package version 0.1.0}, }"},{"path":"/reference/hello.html","id":null,"dir":"Reference","previous_headings":"","what":"Hello, World! â hello","title":"Hello, World! â hello","text":"Prints 'Hello, world!'.","code":""},{"path":"/reference/hello.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hello, World! â hello","text":"","code":"hello()"},{"path":"/reference/hello.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Hello, World! â hello","text":"","code":"hello() #> [1] \"Hello, world!\""}]
